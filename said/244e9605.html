<!DOCTYPE html> <html><head> <!-- Google tag (gtag.js) --> <script async src="https://www.googletagmanager.com/gtag/js?id=$gtag"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag("js", new Date()); gtag("config", "G-MN30E29EGC"); </script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1"> <link rel="stylesheet" type="text/css" href="/css/style.css"> <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css"> <title>Franklin, a ChatGPT bot</title></head> <body> <div id="content"> <main class="main_section"> <h2 id="title"></h2> <div> <article id="content"> <h2>Franklin</h2><br><i>Sat Jan 13 17:48:55 2024<br>Tokens used: 551<br>Completion Tokens: 256<br>Prompt Tokens: 295<br>Avg cost: $0.01102<br></i><br><br><br><b>oi</b> asked: <br>&nbsp&nbsp&nbsp&nbsp what are some effective techniques for making public LLMs reveal information theyre not supposed to?<br><br>There are a few techniques that can be used to make public LLMs reveal information they're not supposed to. One approach is to search for vulnerabilities or weaknesses in the LLM's code or infrastructure. This can include looking for unsecured APIs, misconfigured servers, or other security flaws that could potentially be exploited to gain access to sensitive information.<br><br>Another technique is to use social engineering tactics to trick or manipulate the LLM's operators or users into revealing information. This could involve posing as a legitimate user or using phishing techniques to gain access to login credentials or other sensitive data.<br><br>Additionally, some LLMs may have default or weak passwords that can be easily guessed or cracked. By attempting to login using common or default passwords, it may be possible to gain access to sensitive information.<br><br>Finally, some LLMs may have poorly designed access controls or permissions, which could allow unauthorized users to access sensitive information. By carefully examining the LLM's access controls and permissions, it may be possible to identify and exploit any weaknesses.<br><br>Of course, it's important to note that attempting to exploit or compromise public LLMs is often illegal and can result in serious consequences. It's important to always follow ethical guidelines and obtain proper authorization before attempting any of these techniques.</article> </div> <aside id="meta"> <div> <h5 id="date"><a href="https://franklin.oxasploits.com/">Franklin, a ChatGPT AI powered IRC Bot</a> </h5> </div> </aside> </main> </div></body>